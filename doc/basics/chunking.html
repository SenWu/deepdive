<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="DeepDive" />
    <link href='http://fonts.googleapis.com/css?family=Lato:300,400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="http://deepdive.stanford.edu/stylesheets/application.css" />
    <link rel="canonical" href="http://deepdive.stanford.edu">
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
    <script src="http://deepdive.stanford.edu/javascripts/application.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepDive</title>
  </head>

  <body>
      <script type="text/javascript">
window.analytics||(window.analytics=[]),window.analytics.methods=["identify","track","trackLink","trackForm","trackClick","trackSubmit","page","pageview","ab","alias","ready","group","on","once","off"],window.analytics.factory=function(t){return function(){var a=Array.prototype.slice.call(arguments);return a.unshift(t),window.analytics.push(a),window.analytics}};for(var i=0;i<window.analytics.methods.length;i++){var method=window.analytics.methods[i];window.analytics[method]=window.analytics.factory(method)}window.analytics.load=function(t){var a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src=("https:"===document.location.protocol?"https://":"http://")+"d2dq2ahtl5zl1z.cloudfront.net/analytics.js/v1/"+t+"/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(a,n)},window.analytics.SNIPPET_VERSION="2.0.8",
window.analytics.load("h6uwk48gwg");
window.analytics.page();
</script>
      <a href="https://github.com/hazyresearch/deepdive" target="_blank"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub"></a>

      <div id="header">
        <div class="container">
          <row>
            <div class="col-md-4 col-md-offset-1">
              <a href="http://deepdive.stanford.edu/" class="deepdive-logo">
              <img src="http://deepdive.stanford.edu/images/header_logo.png" style="width: 250px;"/>
              </a> 
            </div>
            <div class="col-md-6 col-md-offset-1">
              <ul class="list-unstyled list-inline" id="header-nav">
                <li><a href="http://deepdive.stanford.edu/index.html">Home</a></li>
                <li><a href="http://deepdive.stanford.edu/doc/basics/installation.html">Download</a></li>
                <li><a href="http://deepdive.stanford.edu/index.html#documentation">Documentation</a></li>
                <li><a href="https://mailman.stanford.edu/mailman/listinfo/deepdive-list" target="_blank">Mailing List</a></li>
              </ul>
              
            </div>
          </row>
        </div>
      </div>

      <section id="main">
        <div class="container">
          <row>
            <div class="col-md-10 col-md-offset-1">
              <h1>Example: Text chunking</h1>

<h2>Introduction</h2>

<p>In this document, we will describe an example application of text chunking using DeepDive. This example assumes a working installation of DeepDive, and basic knowledge of how to build an application in DeepDive. Please go through the <a href="walkthrough/walkthrough.html">example application walkthrough</a> before preceding.</p>

<p>Text chunking consists of dividing a text in syntactically correlated parts of words. For example, the sentence He reckons the current account deficit will narrow to only # 1.8 billion in September . can be divided as follows:</p>

<p>[NP He ] [VP reckons ] [NP the current account deficit ] [VP will narrow ] [PP to ] [NP only # 1.8 billion ] [PP in ] [NP September ] .</p>

<p>Text chunking is an intermediate step towards full parsing. It is the <a href="http://www.cnts.ua.ac.be/conll2000/chunking/">shared task for CoNLL-2000</a>. Training and test data for this task is derived from the Wall Street Journal corpus (WSJ), which includes words, part-of-speech tags, and chunking tags.</p>

<p>In the example, we will predicate chunk label for each word. We include three inference rules, corresponding to logistic regression, linear-chain conditional random field (CRF), and skip-chain conditional random field. The features and rules we use are very simple, just to illustrate how to use multinomial variables and factors in DeepDive to build applications.</p>

<h2>Running the Example</h2>

<p>The full example is under <code>examples/chunking</code> directory. The structure of this directory is</p>

<ul>
<li><code>data</code> contains training and testing data</li>
<li><code>udf</code> contains extractor for extracting training data and features</li>
<li><code>result</code> contains evaluation scripts and sample results</li>
</ul>

<p>To run this example, perform the following steps:</p>

<ol>
<li>Run <code>run.sh</code></li>
<li>Run <code>result/eval.sh</code> to evaluate the results</li>
</ol>

<h2>Example Walkthrough</h2>

<p>The application performs the following high-level steps:</p>

<ol>
<li>Data preprocessing: load training and test data into database.</li>
<li>Feature extraction: extract surrounding words and their part-of-speech tags as features.</li>
<li>Statistical inference and learning</li>
<li>Evaluate results</li>
</ol>

<h3>Data Preprocessing</h3>

<p>The train and test data consist of words, their part-of-speech tag and the chunk tags as derived from the WSJ corpus. The raw data is first copied into into table <code>words_raw</code>, and then is processed to convert the chunk labels to integer indexes. This extractor is defined in <code>application.conf</code> using the following code:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">ext_training {
  input: &quot;select * from words_raw&quot;
  output_relation: &quot;words&quot;
  udf: ${APP_HOME}&quot;/udf/ext_training.py&quot;
}
</code></pre></div>
<p>The input table <code>words_raw</code> looks like</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> word_id |    word    | pos | tag  | id 
---------+------------+-----+------+----
       1 | Confidence | NN  | B-NP |    
</code></pre></div>
<p>The output table <code>words</code> looks like</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> sent_id | word_id |    word    | pos | true_tag | tag | id 
---------+---------+------------+-----+----------+-----+----
       1 |       1 | Confidence | NN  | B-NP     |   0 |  0
</code></pre></div>
<p>The user-defined function <code>udf/ext_training.py</code> looks like this:</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#! /usr/bin/env python</span>

<span class="c"># extract training data</span>

<span class="kn">import</span> <span class="nn">json</span><span class="o">,</span> <span class="nn">sys</span>

<span class="n">tagNames</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;NP&#39;</span><span class="p">,</span> <span class="s">&#39;VP&#39;</span><span class="p">,</span> <span class="s">&#39;PP&#39;</span><span class="p">,</span> <span class="s">&#39;ADJP&#39;</span><span class="p">,</span> <span class="s">&#39;ADVP&#39;</span><span class="p">,</span> <span class="s">&#39;SBAR&#39;</span><span class="p">,</span> <span class="s">&#39;O&#39;</span><span class="p">,</span> <span class="s">&#39;PRT&#39;</span><span class="p">,</span> <span class="s">&#39;CONJP&#39;</span><span class="p">,</span> <span class="s">&#39;INTJ&#39;</span><span class="p">,</span> <span class="s">&#39;LST&#39;</span><span class="p">,</span> <span class="s">&#39;B&#39;</span><span class="p">,</span> <span class="s">&#39;&#39;</span><span class="p">]</span>
<span class="n">sentID</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c"># for each word</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="p">:</span>
  <span class="n">obj</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>

  <span class="c"># get tag</span>
  <span class="c"># extractor bug!</span>
  <span class="k">if</span> <span class="s">&#39;tag&#39;</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">tag</span> <span class="o">=</span> <span class="n">obj</span><span class="p">[</span><span class="s">&#39;tag&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tag</span> <span class="o">!=</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">tag</span> <span class="o">!=</span> <span class="s">&#39;O&#39;</span><span class="p">):</span> 
      <span class="n">tag</span> <span class="o">=</span> <span class="n">tag</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;-&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">tag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tagNames</span><span class="p">:</span> 
      <span class="n">tag</span> <span class="o">=</span> <span class="s">&#39;&#39;</span>

    <span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span>
      <span class="s">&#39;sent_id&#39;</span> <span class="p">:</span> <span class="n">sentID</span><span class="p">,</span>
      <span class="s">&#39;word_id&#39;</span> <span class="p">:</span> <span class="n">obj</span><span class="p">[</span><span class="s">&#39;word_id&#39;</span><span class="p">],</span>
      <span class="s">&#39;word&#39;</span>    <span class="p">:</span> <span class="n">obj</span><span class="p">[</span><span class="s">&#39;word&#39;</span><span class="p">],</span>
      <span class="s">&#39;pos&#39;</span>     <span class="p">:</span> <span class="n">obj</span><span class="p">[</span><span class="s">&#39;pos&#39;</span><span class="p">],</span>
      <span class="s">&#39;true_tag&#39;</span><span class="p">:</span> <span class="n">obj</span><span class="p">[</span><span class="s">&#39;tag&#39;</span><span class="p">],</span>
      <span class="s">&#39;tag&#39;</span>     <span class="p">:</span> <span class="n">tagNames</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
    <span class="p">})</span>

  <span class="k">else</span><span class="p">:</span>
    <span class="n">sentID</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span>
      <span class="s">&#39;sent_id&#39;</span> <span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
      <span class="s">&#39;word_id&#39;</span> <span class="p">:</span> <span class="n">obj</span><span class="p">[</span><span class="s">&#39;word_id&#39;</span><span class="p">],</span>
      <span class="s">&#39;word&#39;</span>    <span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
      <span class="s">&#39;pos&#39;</span>     <span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
      <span class="s">&#39;true_tag&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
      <span class="s">&#39;tag&#39;</span>     <span class="p">:</span> <span class="n">tagNames</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s">&#39;&#39;</span><span class="p">)</span>
    <span class="p">})</span>
</code></pre></div>
<h3>Feature Extraction</h3>

<p>To predict chunking label, we need to add features. We use three simple features: the word itself, its part-of-speech tag, and the part-of-speech tag of its previous word. We add an extractor in <code>application.conf</code></p>
<div class="highlight"><pre><code class="language-text" data-lang="text">ext_features.input: &quot;&quot;&quot;
  select w1.word_id as &quot;w1.word_id&quot;, w1.word as &quot;w1.word&quot;, w1.pos as &quot;w1.pos&quot;, 
    w2.word as &quot;w2.word&quot;, w2.pos as &quot;w2.pos&quot;
  from words w1, words w2
  where w1.word_id = w2.word_id + 1 and w1.word is not null&quot;&quot;&quot;
ext_features.output_relation: &quot;word_features&quot;
ext_features.udf: ${APP_HOME}&quot;/udf/ext_features.py&quot;
ext_features.dependencies: [&quot;ext_training&quot;]
</code></pre></div>
<p>where the input is generating 2-grams from <code>words</code> table, which looks like</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> w1.word_id | w1.word | w1.pos | w2.word | w2.pos 
------------+---------+--------+---------+--------
         15 | figures | NNS    | trade   | NN
</code></pre></div>
<p>The output will look like</p>
<div class="highlight"><pre><code class="language-text" data-lang="text"> word_id |   feature    | id 
---------+--------------+----
      15 | word=figures |   
      15 | pos=NNS      |   
      15 | prev_pos=NN  |   
</code></pre></div>
<p>The user-defined function is</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#! /usr/bin/env python</span>

<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">def</span> <span class="nf">tostr</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
  <span class="k">return</span> <span class="s">&#39;&#39;</span> <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

<span class="c"># for each word</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="p">:</span>
  <span class="n">obj</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>

  <span class="n">features</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="c"># sys.stderr.write(str(obj))</span>

  <span class="c"># features</span>
  <span class="n">w1_word</span> <span class="o">=</span> <span class="s">&#39;word=&#39;</span> <span class="o">+</span> <span class="n">tostr</span><span class="p">(</span><span class="n">obj</span><span class="p">[</span><span class="s">&quot;w1.word&quot;</span><span class="p">])</span>
  <span class="n">w1_pos</span> <span class="o">=</span> <span class="s">&#39;pos=&#39;</span> <span class="o">+</span> <span class="n">tostr</span><span class="p">(</span><span class="n">obj</span><span class="p">[</span><span class="s">&quot;w1.pos&quot;</span><span class="p">])</span>

  <span class="k">if</span> <span class="s">&#39;w2.word&#39;</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">w2_word</span> <span class="o">=</span> <span class="s">&#39;prev_word=&#39;</span> <span class="o">+</span> <span class="n">tostr</span><span class="p">(</span><span class="n">obj</span><span class="p">[</span><span class="s">&quot;w2.word&quot;</span><span class="p">])</span>
    <span class="n">w2_pos</span> <span class="o">=</span> <span class="s">&#39;prev_pos=&#39;</span> <span class="o">+</span> <span class="n">tostr</span><span class="p">(</span><span class="n">obj</span><span class="p">[</span><span class="s">&quot;w2.pos&quot;</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">w2_word</span> <span class="o">=</span> <span class="s">&#39;prev_word=&#39;</span>
    <span class="n">w2_pos</span> <span class="o">=</span> <span class="s">&#39;prev_pos=&#39;</span>
  <span class="c">#w3_word = &#39;next_word=&#39; + tostr(obj[&quot;words_raw.w3.word&quot;])</span>
  <span class="c">#w3_pos = &#39;next_pos=&#39; + tostr(obj[&quot;words_raw.w3.pos&quot;])</span>

  <span class="n">features</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">w1_word</span><span class="p">)</span>
  <span class="n">features</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">w1_pos</span><span class="p">)</span>
  <span class="n">features</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">w2_pos</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="k">print</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span>
      <span class="s">&quot;word_id&quot;</span><span class="p">:</span> <span class="n">obj</span><span class="p">[</span><span class="s">&quot;w1.word_id&quot;</span><span class="p">],</span>
      <span class="s">&#39;feature&#39;</span><span class="p">:</span> <span class="n">f</span>
    <span class="p">})</span>
</code></pre></div>
<h3>Statistical Learning and Inference</h3>

<p>We will predicate the chunk tag for each word, which corresponds to <code>tag</code> column of <code>words</code> table. The variables are declared in <code>application.conf</code>:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">schema.variables {
  words.tag: Categorical(13)
}
</code></pre></div>
<p>Here, we have 13 types of chunk tags <code>NP, VP, PP, ADJP, ADVP, SBAR, O, PRT, CONJP, INTJ, LST, B, null</code> according to CoNLL-2000 task description. We have three rules, logistic regression, linear-chain CRF, and skip-chain CRF. The logistic regression rule is</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">factor_feature {
  input_query: &quot;&quot;&quot;select words.id as &quot;words.id&quot;, words.tag as &quot;words.tag&quot;, word_features.feature as &quot;feature&quot; 
    from words, word_features 
    where words.word_id = word_features.word_id and words.word is not null&quot;&quot;&quot;
  function: &quot;Multinomial(words.tag)&quot;
  weight: &quot;?(feature)&quot;
}
</code></pre></div>
<p>To express conditional random field, just use <code>Multinomial</code> factor to link variables that could interact with each other (For more information about CRF, see <a href="http://people.cs.umass.edu/%7Emccallum/papers/crf-tutorial.pdf">this tutorial on CRF</a>. The following rule links labels of neiboring words</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">factor_linear_chain_crf {
  input_query: &quot;&quot;&quot;select w1.id as &quot;words.w1.id&quot;, w2.id as &quot;words.w2.id&quot;, w1.tag as &quot;words.w1.tag&quot;, w2.tag as &quot;words.w2.tag&quot;
    from words w1, words w2
    where w2.word_id = w1.word_id + 1&quot;&quot;&quot;
  function: &quot;Multinomial(words.w1.tag, words.w2.tag)&quot;
  weight: &quot;?&quot;
}
</code></pre></div>
<p>It is similar with skip-chain CRF, where we have skip edges that link labels of identical words.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">factor_skip_chain_crf {
  input_query: &quot;&quot;&quot;select *
  from
    (select w1.id as &quot;words.w1.id&quot;, w2.id as &quot;words.w2.id&quot;, w1.tag as &quot;words.w1.tag&quot;, w2.tag as &quot;words.w2.tag&quot;,
      row_number() over (partition by w1.id) as rn
    from words w1, words w2
    where w1.tag is not null and w1.sent_id = w2.sent_id and w1.word = w2.word and w1.id &lt; w2.id) scrf
  where scrf.rn = 1&quot;&quot;&quot; 
  function: &quot;Multinomial(words.w1.tag, words.w2.tag)&quot;
  weight: &quot;?&quot;
}
</code></pre></div>
<p>We also specify the holdout variables according to task description about training and test data.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">calibration: {
  holdout_query: &quot;INSERT INTO dd_graph_variables_holdout(variable_id) SELECT id FROM words WHERE word_id &gt; 220663&quot;
}
</code></pre></div>
<h3>Evaluation Results</h3>

<p>Running <code>result/eval.sh</code> will give the evaluation results. Below are results for using different rules. We can see that by adding CRF rules, we get better results both for precision and recall.</p>

<p>Logistic Regression
  processed 47377 tokens with 23852 phrases; found: 23642 phrases; correct: 19156.
  accuracy:  89.56%; precision:  81.03%; recall:  80.31%; FB1:  80.67
               ADJP: precision:  50.40%; recall:  42.92%; FB1:  46.36  373
               ADVP: precision:  69.21%; recall:  71.13%; FB1:  70.16  890
              CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  13
               INTJ: precision: 100.00%; recall:  50.00%; FB1:  66.67  1
                LST: precision:   0.00%; recall:   0.00%; FB1:   0.00  0
                 NP: precision:  79.88%; recall:  77.52%; FB1:  78.68  12055
                 PP: precision:  90.51%; recall:  89.59%; FB1:  90.04  4762
                PRT: precision:  66.39%; recall:  76.42%; FB1:  71.05  122
               SBAR: precision:  83.51%; recall:  71.96%; FB1:  77.31  461
                 VP: precision:  79.48%; recall:  84.71%; FB1:  82.01  4965</p>

<p>LR + Linear-Chain CRF
  processed 47377 tokens with 23852 phrases; found: 22996 phrases; correct: 19746.
  accuracy:  91.58%; precision:  85.87%; recall:  82.79%; FB1:  84.30
                   : precision:   0.00%; recall:   0.00%; FB1:   0.00  1
               ADJP: precision:  75.74%; recall:  69.86%; FB1:  72.68  404
               ADVP: precision:  76.47%; recall:  73.56%; FB1:  74.99  833
              CONJP: precision:  25.00%; recall:  22.22%; FB1:  23.53  8
               INTJ: precision:  50.00%; recall:  50.00%; FB1:  50.00  2
                LST: precision:   0.00%; recall:   0.00%; FB1:   0.00  0
                 NP: precision:  82.22%; recall:  77.19%; FB1:  79.63  11662
                 PP: precision:  93.43%; recall:  94.26%; FB1:  93.84  4854
                PRT: precision:  66.67%; recall:  69.81%; FB1:  68.20  111
               SBAR: precision:  84.93%; recall:  74.77%; FB1:  79.52  471
                 VP: precision:  90.37%; recall:  90.21%; FB1:  90.29  4650</p>

<p>LR + Linear-Chain CRF + Skip-Chain CRF
  processed 47377 tokens with 23852 phrases; found: 22950 phrases; correct: 19794.
  accuracy:  91.79%; precision:  86.25%; recall:  82.99%; FB1:  84.59
                   : precision:   0.00%; recall:   0.00%; FB1:   0.00  1
               ADJP: precision:  75.25%; recall:  68.72%; FB1:  71.84  400
               ADVP: precision:  76.29%; recall:  73.56%; FB1:  74.90  835
              CONJP: precision:  30.00%; recall:  33.33%; FB1:  31.58  10
               INTJ: precision: 100.00%; recall:  50.00%; FB1:  66.67  1
                LST: precision:   0.00%; recall:   0.00%; FB1:   0.00  0
                 NP: precision:  82.96%; recall:  77.54%; FB1:  80.16  11611
                 PP: precision:  93.70%; recall:  94.30%; FB1:  94.00  4842
                PRT: precision:  66.67%; recall:  69.81%; FB1:  68.20  111
               SBAR: precision:  83.37%; recall:  74.95%; FB1:  78.94  481
                 VP: precision:  90.34%; recall:  90.34%; FB1:  90.34  4658</p>

            </div>
          </row>
        </div>
      </section>
    
      <footer id="footer">
        <div class="container">
          <row>
            <div class="col-md-10 col-md-offset-1">
              <p class="pull-left"> 
                Copyright, 2014 deepdive.stanford.edu
                ⋅
                <a href="mailto:contact.hazy@gmail.com">Questions? Email us</a>
              </p>
              <p class="pull-right"> 
                Visit DeepDive on <a href="https://github.com/hazyresearch/deepdive" target="_blank">Github</a> 
              </p>
            </div>
          </row>
        </div>
      </footer>

    
  
  </body>
</html>
